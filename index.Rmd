---
title: "Introduction à la statistique sous R"
subtitle: "Mise à niveau M2 Géoprisme 2022"
date: "`r Sys.Date()`"
author: 
 - name: Claude Grasland
   affiliation: Université de Paris (Diderot), UMR 8504 Géographie-cités, FR 2007 CIST
image: "figures/Lego1.jpg"   
logo: "figures/Lego1.jpg"  
output:
  rzine::readrzine:
    highlight: kate
    number_sections: true
csl: Rzine_citation.csl
bibliography: biblio.bib
nocite: |
  @*
link-citations: true
#licence: "[![licensebuttons by-sa](https://licensebuttons.net/l/by-sa/3.0/88x31.png)](https://creativecommons.org/licenses/by-sa/4.0)"
#giturl: "[![Code source on GitHub](https://badgen.net/badge/Code%20source%20on%20/GitHub/blue?icon=github)](xxx)"
#doi: "[![DOI:xxx](https://zenodo.org/badge/DOI/xxx.svg)](https://doi.org/xxx)"
---


```{r setup, include=FALSE}

library(knitr)
library(rzine)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="bg-info",
               class.output="bg-warning")

# opts_knit$set(width=75)
```



# INITIATION A R-BASE {-}

Cette initiation est destinée à des étudiants n'ayant jamais utilisé R mais connaissant à peu près la statistique univariée et bivariée...

L'idée pédagogique est d'apprendre directement aux étudiants à programmer en R markdown plutôt qu'en R. Pourquoi ? Parce qu'ainsi ils vont simultanément :

- taper du code R qu'ils ignorent
- écrire sous ce code les explications du point de vue informatique
- observer les résultats statistiques
- interpréter ces résultats d'un point de vue statistique

Cela n'a l'air de rien, mais en procédant ainsi les étudiants apprennent à produire à la fois leurs notes de cours en R, leurs notes de cours en statistiques et ... le langage Rmarkdown pour rédiger leurs futurs travaux.

Bref, si tout a bien marché, l'étudiant n'aura même pas besoin de consulter le présent document, si ce n'est pour vérifier que son programme donne les mêmes résultats ...


Les exercices qui suivent utilisent volontairement les fonctions de base du langage R (on dit que l'on *programme en R-base*) à l'exclusion de tout *package* c'est-à-dire de tout outil graphique ou statistique mis au point ultérieurement. 

Par comparaison avec le jeu de lego, cela revient à effectuer des constructions avec la boîte de base. A première vue cela peut sembler frustrant. Mais en réalité cela ne bride en rien l'imagination et permet d'apprendre plein de choses sans être distrait ...

## Données et exercices

Vous pouvez télécharger les fichiers de données et d'exercice en cliquant sur [ce lien](https://github.com/ClaudeGrasland/Init_R_2021/raw/main/exos.zip).


# POURQUOI R ? 



## Rstudio et les projets R

>- Au commencement, les dieux de la statistique créèrent le langage R.
>- Mais l'interface était vide et vague, 
>- les ténèbres couvraient les lignes de code
>- R-Studio dit : Que le projet soit et le projet fut. 



Si l'on veut s'épargner bien des désagréments dans l'apprentissage de R, il faut prendre dès le départ de bonnes habitudes. Parmi celles-ci, l'une des plus importantes est le fait d'inscrire toujours son travail dans le cadre d'un **projet R** c'est-à-dire - en simplifiant beaucoup - un **répertoire de travail** contenant l'ensemble des données, programmes, résultats... que l'on pourra par la suite compresser, archiver et transmettre à quelqu'un d'autre. 

### Lancement de R studio

Sauf à être complètement masochiste, on n'utilise jamais **R** directement mais on lance d'abord l'interface **R-Studio** qui facilite conisdérablement l'ensemble des opérations et offre une gamme considérable de services. Il ne faut toutefois pas confondre les deux et il serait par exemple ridicule d'indiquer sur un CV en vue d'un emploi de statisticien que l'on sait utiliser R-studio en oubliant de préciser que l'on maîtrise R. 


### Création d'un projet 

Pour créer un projet on utilise le menus déroulant *File/new project/ ...* et on définit un dossier de notre ordinateur (existant ou à créer) qui contiendra le projet. Une fois l'opération effectuée, on pourra constater que ce dossier contient un fichier *xxx.Rproj* ou xxx est en principe le nom du dossier dans lequel vous avez stocké le projet.

Ce fichier contient toute une série d'informations dont nous ne parlerons pas dans ce cours d'initiation mais qui, pour faire simple, définissent un ensemble de réglages du logiciel et de préférences de l'utilisateur. 

Si vous fermez Rstudio (faites-le !) il vous suffira pour reprendre votre travail là où vous vous étiez arrêté :

- de lancer R-studio et de cliquer sur *File/open project/...* suivi du nom du fichier xxx.Rproj
- ou plus simplement encore de double cliquer sur le fichier xxx.Rproj ce qui lancera automatiquement Rstudio

Le dossier contenant votre projet R peut être organisé à votre convenance. Certains mettent tout les fichier pêle-mêle dans le dossier. D'autres préfèrent créer des sous-dossiers contenant des données, des programmes, des résultats, des figures. Vous déciderez à l'usage de ce qui vous convient le mieux, mais le point important est que **tout ce qui entre ou sort de vos programmes R doit être de préférence stocké dans le répertoire du projet**. 

```{r, echo = FALSE}
knitr::include_graphics("figures/monprojet.png")
```


## Programme R : Excel killer ?

>- C'est pourquoi tu quittera Word et Excel, et t'attachera à R studio,
>- et vous deviendrez une seule chair. 



La fonction initiale d'un langage de programmation comme R est ... de créer des **programmes** c'est-à-dire des ensembles d'instruction permettant d'accomplir une tâche à l'intérieur d'une chaîne de production. Dans le cas d'un logiciel spécialisé dans l'analyse statistique, il s'agira donc de partir de données (statistiques, géographiques, textuelles, ...) pour aboutir à des résultats prenant la forme de tableaux, cartes ou graphiques. Il ne s'agit donc en somme que d'une étape du travail de recherche où le principal avantage de R est d'**automatiser une tâche** et de faciliter sa **reproduction ultérieure** avec en arrière plan un objectif de **productivité** puisque l'ordinateur réalise en quelques millisecondes des tâches qui prendraient des heures avec un logiciel click-bouton de type Excel. 


```{r, echo = FALSE}
knitr::include_graphics("figures/ProgrammeR.png")
```

Prenons un exemple simple de problème facile à résoudre avec R mais plus compliqué avec des logiciels click-boutons. Il s'agit d'un exemple pédagogique tiré d'un [très vieux cours d'analyse spatiale](!http://grasland.script.univ-paris-diderot.fr/go303/ch1/ch1.htm) portant sur les semis de point et les localisations optimales.

On considère une carte papier permettant de localiser 5 station services à l'intérieur d'une ville à plan en damier. Chaque station livre chacune la même quantité de carburant par semaine aux clients. On souhaite répondre aux questions suivantes :

1. Comment saisir les données dans une fichier numérique ?
2. Comment reproduire la carte papier sous forme d'un graphique ?
3. Comment calculer la dsitance à vol d'oiseau entre toutes les stations ?
4. Comment calculer la dsitance routière entre toutes les stations ?
5. Où localiser un dépôt de carburant permettant d'alimenter les cinq stations en minimisant la distance moyenne de livraison (critère d'efficacite)
6. Où localiser une caserne de pompier qui doit pouvoir intervenir rapidement sur toute les stations et qui doit minimiser la distance maximale à la station la plus éloignée (critère d'équité).
7. Comment visualiser ces deux localisations sur la carte des stations ?
8. Comment reproduire ces tâches rapidement s'il y a des ajouts ou suppressions de stations

```{r echo=FALSE}
knitr::include_graphics("figures/access.png")
```

On constitue deux équipes d'étudiants, certains utilisant un programme R et d'autres Excel. On se propose de voir qui ira le plus vite sur chacune des 8 tâches proposées. 


### Round 1. Saisie des données et affichage du tableau

On crée un programme R avec *File/New File/R Script* puis on l'enregistre avec *File/Save/ ...* suivi du nom du programme.

```{r}
# Saisie des variables
CODE <- c("A","B","C","D","E")
X <- c(10,20,40,50,180)
Y <- c(40,60,40,60,50)

# Regroupement dans un tableau
coo <- data.frame(X,Y)

# Ajout du nom des lignes
row.names(coo) <- CODE

# Affichage du tableau
coo
```
Normalement, les étudiants qui utilisent un tableur ont du aller plus vite et Excel mène sur R par 1-0


### Round 2. Affichage de la carte

Vous devez essayez de reproduire l'image correspondant au problème posé 

```{r}
plot(X,Y, 
     col="red", 
     pch=20, 
     xlim=c(0,180),
     ylim=c(0,90),
     asp = 1)
text(X,Y,
     labels = CODE, 
     pos = 2)

```
La création d'un graphique est à première vue plus facile avec un logiciel click-bouton. L'avantage est très clairement pour Excel qui mène désormais 2 à 0.

### Round 3. Calcul de la station la plus accessible à vol d'oiseau (distance euclidienne)

Vous devez calculer une matrice de **distance euclidienne** entre toutes les stations et trouver la plus accessible.

```{r}

# calcul la matrice de distance euclidienne
mat<-dist(coo, upper = T, method = "euclidean")
mat

# distance moyenne
apply(as.matrix(mat),1,mean)
```
Là, je parie que les utilisateurs d'Excel ont eu un peu plus de mal ... En tous les cas, Excel ne mèneplus que par 2 à 1


### Round 4. Calcul de la station la plus accessible par la route (distance de Manhattan)

Vous devez calculer une matrice de distance de Manhattan entre toutes les stations et trouver la plus accessible.

```{r}

# calcul la matrice de distance de Manhattan
mat<-dist(coo,upper = T, method = "manhattan")
mat

# distance moyenne de Manhattan
apply(as.matrix(mat),1,mean)
```

Je reconnais que c'est unpeu facile, mais à nouveau R l'emporte ce qui fait désormais match nul 2-2


### Round 5. Localisation du dépôt de carburant

Dans le cas particulier de la distance de Manhattan, le calcul du point le plus proche de tous les autres s'obtient facilement en calculant le point médian dont les coordonnées correspondent à la médiane de X et la médiane de Y.

```{r}
medX <- median(X)
medX
medY <- median(Y)
medY

```

A priori, le calcul est aussi facile dans R et dans Excel : match nul 3-3


### Round 6. Localisation de la caserne de pompiers

Dans le cas particulier de la distance de Manhattan, le calcul du point minimisant la distance maximale s'obtient en trouvant le centre du diamètre minimal en X et en Y. Il s'agit de la localisation la plus **équitable** où le plus défavorisé est le moins défavorisé possible. 

```{r}
equX <- (max(X)+min(X))/2
equX
equY <- (max(Y)+min(Y))/2
equY


```

A priori, le calcul est toujours aussi facile dans R et dans Excel : match nul 4-4


### Round 7. Visualisation des deux points sur la carte

On va placer en bleu le point médian et en vert le point le plus équitable. Dans le cas de R on peut recopier les lignes de code du graphique du round n°2 ce qui gagne désormais du temps :

```{r}
# Programme antérieur
plot(X,Y, 
     col="red", 
     pch=20, 
     xlim=c(0,180),
     ylim=c(0,90),
     asp = 1)
text(X,Y,
     labels = CODE, 
     pos = 2)

# Ajout du dépôt de carburant
points(medX, medY, col="blue", pch=3)
text(medX,medY, "dépot",pos=1)

# Ajout du point médian
points(equX, equY, col="green", pch=3)
text(equX,equY, "caserne",pos=1)



```

Le résultat du match est incertain mais R n'est plus désavantagé puisqu'on peut recycler les lignes de code précédentes pour le graphique de base. Disons 5-5 même s'il y a de fortes chances que R l'emporte.


### Dernier round. Refaire toute l'analyse avec une station de plus

Deux stations  F(100,20) et G(150,30) ont été ajoutées. Il faut refaire la carte finale. Cela ne pose aucun problème dans R puisqu'il suffit de modifier l'entrée des données  et récupérer des bouts de programme

```{r}
# (1) Saisie des variables
CODE <- c("A","B","C","D","E","F","G")
X <- c(10,20,40,50,180,100,150)
Y <- c(40,60,40,60,50,20,30)
coo <- data.frame(X,Y)
row.names(coo) <- CODE


# (2) calcul des points centraux
medX <- median(X)
medY <- median(Y)
equX <- (max(X)+min(X))/2
equY <- (max(Y)+min(Y))/2


# (3) Graphique
plot(X,Y, 
     col="red", 
     pch=20, 
     xlim=c(0,180),
     ylim=c(0,90),
     asp = 1)
text(X,Y,
     labels = CODE, 
     pos = 2)

# Ajout du dépôt de carburant
points(medX, medY, col="blue", pch=3)
text(medX,medY, "Dépôt",pos=1)

# Ajout du point médian
points(equX, equY, col="green", pch=3)
text(equX,equY, "Caserne",pos=1)


```

Excel n'a aucune chance d'aller plus vite et R remporte le match par KO !


## Document Rmd : Word killer ?


>- R-Studio dit : « Faisons une interface de rédaction adaptée à notre travail, 
>- Que l'utilisateur puissent y insérer les tableaux, les graphiques, les cartes, les références bibliographiques, et tous les écrits qui les commentent. »


Nous venons de voir comment une bonne pratique de R peut conduire progressivement à abandonner l'usage des tableurs (Excel, Open Office) sauf peut-être pour l'étape de saisie des données. Dès lors qu'il s'agit de réaliser des graphiques ou des calculs statistiques complexes, la rédaction d'un programme se révèle beaucoup plus intéressante même si elle impose un coût initial d'apprentissage.

Mais une bonne pratique de R ou plus précisément des documents  R markdown peut vous conduire beaucoup plus loin et vous amener à abandonner également votre logiciel de traitement de texte (Word) et votre outil de présentation (Power Point). Le coût d'apprentissage est naturellement un peu plus élevé mais les bénéfices sont à la mesure de l'investissement.

Comme le montre la figure ci-dessous, un document R markdown est en quelques sorte un mélange entre des lignes de code R qui executent des tâches et des lignes de texte où sont expliqués les calculs et commentés les résultats obtenus. En d'autres termes, un document R markdown vous permet de rédiger un article de recherche complet, une présentation à une conférence, un syllabus de cours, dans un seul environnement logiciel (R studio). 

Nul besoin de ciseau et de colle pour aller chercher tel tableau ici, tel figure là-bas ou telle carte ailleurs. Tous ces éléments sont intégrs au fur et à mesure de la rédaction ce qui facilite considérablement la concentration. Et surtout - on l'a déjà vu pour le programme R - le document peut facilement être reproduit ou mise à jour sans être obligé de réplique des dizaines de click de souris.   


```{r echo=FALSE}
knitr::include_graphics("figures/DocumentRmd.png")
```
Nous allons illustrer l'usage de R markdown en rédigeant une courte note sur la distribution de la population et de certains commerces et services à Rennes. L'exemple est repris du [Manuel d'analyse spatiale publié par l'INSEE en 2018 ](https://www.insee.fr/fr/information/3635442) et plus précisement de son [chapitre 4. Les configurations de points](https://www.insee.fr/fr/statistiques/fichier/3635442/imet131-h-chapitre-4.pdf) 

Comme nous avons pris la perspective de n'employer aucun package R au cours de cette formation initiale, les données ont été légèrement modifiées, notamment pour le tracé de la carte des contours de la ville de Rennes.


### Chargement des données

Nous disposons de trois fichiers qui comportent chacun des coordonnées de localisation utilisant la même projection Lambert et que l'on pourra de ce fait superposer. Après les avoir chargés et décrits, on en propose une première visualisation à l'aide des fonctions graphiques de base de R.


#### Contour de Rennes

On charge le fichier avec `read.table()` puis on affiche ses premières lignes avec `head()`et on regarde sa taille avec `dim()`

```{r}
map <- read.table(file = "data/rennes/map.csv",
                  header = T,
                  sep = ";")
head(map,2)
dim(map)
```
On affiche le contour avec les instructions `plot()` et `lines()`. On doit impérativement ajouter le paramètre *asp = 1* dans plot() pour imposer une échelle identique sur l'axe vertical et l'axe horizontal. 

```{r}
plot(map$x,map$y, col="red", asp = 1)
lines(map$x,map$y, col="blue")
```

#### Distribution de la population

On charge le fichier de population de la même manière et on constate qu'il comporte une troisième colonne indiquant la population localisée en chaque point. En fait, il s'agit d'une grille de population qui localise les habitants sur une maille de ??? m 

```{r}
pop <- read.table(file = "data/rennes/popu.csv",
                  header = T,
                  sep = ";")
head(pop,2)
dim(pop)
```
On procède à une première cartographie qui ne tient pas compte de l'effectif de population mais indique juste les cases occupées et inoccupées, ce qui permet de donner une vision générale de l'occuparion du sol et du peuplement de Rennes et de l'espace environnant. 

```{r}
plot(pop$x,pop$y, col="red", asp = 1,pch=22, cex=0.01)
lines(map$x,map$y, col="black")

```
#### Distribution des équipements

L'INSEE a extrait du fichier de la Base Publique des Equipements quatre types de localisations correspondant aux écoles, aux médecins, aux pharmacies et aux commerces de vêtements. On notera l'ajout du paramètre *encoding="UTF-8"* qui permet de lire sans erreur les caractères accentués et d'éviter par exemple que "Vêtements" devienne "VÃªtements".

```{r}
bpe <- read.table(file = "data/rennes/bpe.csv",
                  header = T,
                  sep = ";",
                  encoding="UTF-8")
head(bpe,2)
dim(bpe)
```


On utilise l'instruction `table()`pour dénombrer l'effectif de chaque équipement :

```{r}
table(bpe$equ)
```

Puis on visualise après avoir attribué une couleur à chaque équipement. On crée pour cela une nouvelle variable :

```{r}
bpe$couleur<-as.factor(bpe$equ)
levels(bpe$couleur)
levels(bpe$couleur)<-c("blue","green","orange","red")
bpe$couleur<-as.character(bpe$couleur)
table(bpe$couleur)
```

On peut désormais assembler nos trois couches : 

```{r}
plot(pop$x,pop$y, col="gray", asp = 1,pch=22, cex=0.01)
lines(map$x,map$y, col="black")
points(bpe$x,bpe$y,bg=bpe$couleur, pch=21, cex=0.8)
```

Il est facile de procéder à un zoom en ajoutant des paramètres *xlim* et *ylim* dans la fonction plot() qui précise l'espace d'étude. 

```{r}
plot(pop$x,
     pop$y, 
     col="gray", 
     asp = 1,
     pch=22, 
     cex=0.1,
     xlim = c(351000,353000),
     ylim = c(6788500,6790500))

lines(map$x,
      map$y, 
      col="black")

points(bpe$x,
       bpe$y,
       bg=bpe$couleur, 
       pch=21, 
       cex=0.8)
```


OK, notre carte n'a pas de légende (c'est possible mais vraiment compliqué en R-Base) mais on appréciera le fait d'avoir pu la réaliser en ne se servant que de quelques fonctions élémentaires de R comme 

### A vous de jouer !

En vous inspirant de l'exemple des stations services, essayez de définir un certain nombre de questions et tentez d'y répondre ...




# STAT. BIVARIEE : Y Quantitative 

Accrochez vos ceintures … on plonge directement dans un programme R qu’il s’agit juste d’exécuter sans comprendre ce qui se passe pour avoir une idée des possibilités du logiciel mais aussi des difficultés. Vous êtes un peu comme un étranger qui entend parler une langue nouvelle et découvre de nouveaux mots et essaye de les reproduire comme dans la méthode Assimil … Evidemment ça ne marche pas à tous les coups. 

On commence ici par un exemple où tout se passe bien et on utilise pour cela un exemple fétiche de l'enseignant, celui des pays d'Europe en 1988 à la veille de la chute du mur de Berlin. Exemple qui a été testé depuis 30 ans sur tous les logiciels de statistique possible (Lotus, Calc, SAS, XLSTAT, SPAD, ....). Il vous est recommandé d'avoir également un exemple fétiche sur lequel vous pourrez tester à votre tour le programme ci-dessous en l'adaptant.



```{r}
knitr::include_graphics("figures/coldwar.png")
```


Les 24 pays d'Europe sont identifiés par leur code iso (PAYS), leur appartenance aux pays socialistes ou capitalistes (BLOC), leur position spatiale (X,Y) et tout un ensemble de variables économpiques et sociale telles que le PNB par habitant (PNB), le taux de mortlaité infantile (TMI), l'espérance de vie à la naissance (ESP), le taux d'urbanisation (URB), le taux de natalité (NAT), le taux de mortalité (MOR), l'indice conjoncturel de fécondité (FEC), le \% de jeunes de 0-14 ans (JEU), le \% de vieux de 65 ans et plus (VIE), la superficie totale en milliers de kM2  (SUP) et la population totale en millions (POP).

## Définir un environnement de travail



### Définir un espace de travail

le répertoire de travail dans lequel seront lus ou écrit les fichiers. 

Dans le programme ci-dessous, les lignes commençant par "#" sont des commentaires. La seule ligne active est getwd() qui indique la position du répertoire courant. Il est recommandé aux débutants, mais aussi aux programmeurs plus expérimentés de ne pas hésiter à ajouter beaucoup de lignes de commentaires. Cela facilité l'apprentissage et permet de transmettre ses programmes à d'autres.  


```{r}
# ---------------------------
# (1) ESPACE DE TRAVAIL
# ---------------------------
# (1.1) Quel est le répertoire actuel ?
getwd()
```

On repère ensuite le chemin de l'emplacement du dossier où se trouvent les données et on examine son contenu. Attention, contrairement à windows il faut toujours utiliser des slash (/) et non pas des antislash pour décrire les chemins d'accès des fichiers


```{r}
# (1.2) Choix du rÈpertoire contenant les donnÈes
monchemin<-"data/euro1988"
list.files(monchemin)

```




Nous allons maintenant essayer de charger le fichier de données 


## Préparer son tableau de données

On utilise ici une procédure adaptée à la lecture de fichiers texte :  *read.table()*
L'instruction *header=TRUE* signale que la première ligne donne le nom des variables
L'instruction *dec="."* signale que les décimales sont représentées par des points et non pas des virgules.


### Importation d'un tableau
```{r}
# --------------------------------------------------------
# (2) IMPORTATION ET MISE EN FORME D'UN TABLEAU DE DONNEES
# --------------------------------------------------------
# (2.1) Importation d'un fichier .txt
euro <- read.table("data/euro1988/euro1988.txt", 
                   dec=".",
                   header=TRUE)
```


Pour savoir si le chargement s'est bien pssé on peut afficher le tableau en tapant son nom
```{r}
euro 
```

Mais on peut aussi se contenter d'afficher les premières lignes (*head*) ou les dernières lignes (*tail*) du tableau. Par exemple, pour voir les trois premières et les 5 dernières lignes : 
```{r}
head(euro,3)
tail(euro,5)
```

Naturellement ... les instructions présentées ci-dessus ne marcheront pas pour tous les tableaux et vous devez vous attendre à pas mal de difficultés à ce stade. Il existe en effet beaucoup d'options à connaître pour charger les fichiers. Vous pouvez ainsi examiner toutes les options de la procédure *read.table* en tapant un ? suivi du nom de la procédure. Et vous pouvez faire de même pour les autres procédures d'importation comme *read.csv*, *read.delim*, etc...

### Choix du nom des lignes

Par défaut, le nom des lignes correspond à l'ordre de lecture du fichier (*1..24*). Mais il peut être intéressant de le préciser en lui attribuant par exemple la valeur d'une variable servant d'identifiant. 

```{r}
rownames(euro)<-euro$PAYS
head(euro)
```

### Vérification du type des variables

On peut s'éviter beaucoup d'ennuis en contrôlant le type des variables qui a été attribué aux différentes colonnes par R lors de la lecture d'un fichier. Pour cela on commence par regarder quels sont les types de variables du tableau à l'aide de l'instruction *str()* :

```{r}
str(euro)
```

L'instruction *str()* est très importante dans R. Elle permet en effet d'examiner le type d'un objet et des éléments qui le composent. Ici on note que l'objet euro est un *data.frame* c'est-à-dire un tableau de données croisant des indivdus (lignes) et des variables de types différents (colonnes). Par défaut, R a attribué le type "Factor" à toutes les variables composées de texte, le type "int" ou integer à toutes les variables comportant uniquement des nombres entiers et enfin le type "num" ou numérique à toutes les variables comportant des chiffres avec des décimales.  Il peut arriver que ce choix par défaut ne correspondent pas aux traitements que l'on veut effectuer et, dans ce cas, on procèdera à des conversions de variables d'un type à un autre. 

Ainsi le type "Factor" est difficile à utiliser pour les débutants et il est souvent confondu avec le type "Character". Pour fixer les idées, on peut dire qu'un "Factor" correspond à ces classes portant à la fois un numéro et une étiquette. Ainsi la variable "BLOC" est bien un facteur correspondant à deux classes : celle des pays capitalistes (numéro = 1, étiquette ="cap") et celle des pays socialistes (numero = 2, étiquette = "soc"). Par contre la variable PAYS ne définit pas des classes puisqu'elle est différente pour chaque pays et est donc du type "character".

De la même manière, un nombre entier peut conduire à des comportements bizarre pour les valeurs élevées et il vaut mieux utiliser le tupe numérique lorsqu'on est en présence de nombre réels qui ont juste été arrondis à zéro chiffres après la virgule. Dans notre exemple, il n'existe pas de variables entières mais uniquement des nombres réels arrondis. 



```{r}
euro$PAYS<-as.character(euro$PAYS)
euro$BLOC<-as.factor(euro$BLOC)
euro$PNB<-as.numeric(euro$PNB)
euro$ESP<-as.numeric(euro$ESP)
euro$URB<-as.numeric(euro$URB)
euro$NAT<-as.numeric(euro$NAT)
euro$MOR<-as.numeric(euro$MOR)
euro$JEU<-as.numeric(euro$JEU)
euro$VIE<-as.numeric(euro$VIE)
euro$SUP<-as.numeric(euro$SUP)
euro$POP<-as.numeric(euro$POP)
euro$X<-as.numeric(euro$X)
euro$Y<-as.numeric(euro$Y)
str(euro)
```

Et voilà : désormais notre tableau ne comporte plus que des variables de type caractère ou de type numérique. Cela nous a demandé un peu de temps mais nous évitera par la suite beaucoup d'ennuis. En effet,  **au moins 50 % des erreurs dans R viennent d'un problème de mauvaise définiton du type d''un objet ou du type d'une variable  !!!**

### Enregistrement du tableau au format propre à R

Il n'est pas forcément necessaire d'enregistrer ses données au format propre à R (*.RDS*)s'il s'agit d'un petit tableau. Il suffit en effet de sauvegarder le programme que nous venons d'écrire et de l'executer chaque fois que nécessaire. Toutefois, il peut arriver que cette étape soit longue et dans ce cas la sauvegarde est utile.

```{r}
save(euro, file="data/euro1988/euro1988.RDS")
```

on pourra ensuite facilement récupérer les données par la commande suivante


```{r}
load("data/euro1988/euro1988.RDS")
```


### Un petit résumé statistique 

N'oublions pas que R est avant tout un logiciel de statistiques... La récompense d'une bonne définition du tableau sera d'obtenir pour chaque variable un résumé adapté à son type :

```{r}
summary(euro)
```




On va tenter de répondre à des questions simples sans détailler dans l'immédiat les outils statistiques qui permettent de valider scientifiquement les réponses. On insistera plus ici sur les capacités de visualisation de résultats sous R.

## Expliquer une variable quantitative (Y) par une variable qualitative (X) 

La mortalité infantile (Y) est-elle significativement plus forte dans les pays socialistes (X)?

Notre objectif est plus généralement de mettre en relation une variable numérique (TMI) avec une variable de type facteur (BLOC). On va procéder par étape en étudiant d'abord chque variable séparément puis en les croisant et finalement en testant notre hypothèse d'une surmortalité infantile des pays socialistes.  

### Analyse de la répartition des pays par bloc politique (BLOC)

Deux petites commandes pour dénombrer l'effectif des classes et les représenter
```{r}
# Analyse de la  variable qualitative (BLOC)
#calcul des fréquences
summary(euro$BLOC)
#diagramme en bâtons
plot(euro$BLOC)
```

### Analyse de la distribution des taux de mortalité infantile (TMI)

Quelques petites commandes pour calculer les paramètres principaux de la mortalité infantile et la représenter.

```{r}
# Analyse de la variable quantitative (TMI)

# calcul et visualisation des quantiles
summary(euro$TMI)
boxplot(euro$TMI)

#calcul de la moyenne et de l'écart type
mean(euro$TMI)
sd(euro$TMI)

# visualisation de l'histogramme
hist(euro$TMI)
```

### Comparaison de la mortalité infantile par bloc (BLOC*TMI)

Quelques outils pour analyser conjointement les deux distributions.

```{r}
# calcul et visualisation des quantiles
tapply(euro$TMI, euro$BLOC, summary)
boxplot(euro$TMI~euro$BLOC)

#calcul de la moyenne et de l'écart type
tapply(euro$TMI, euro$BLOC, mean)
tapply(euro$TMI, euro$BLOC, sd)

# visualisation des deux histogrammes
par(mfrow= c(2,1)) 
hist(euro$TMI[euro$BLOC =="Soc"],
    xlim = c(0,50),
     main="Pays Socialistes", 
     col="red") 
hist(euro$TMI[euro$BLOC =="Cap"],
    xlim = c(0,50),
     main="Pays Capitalistes",
     col="blue") 

```


### Test d'égalité des moyennes (BLOC*TMI)

Maintenant que nous avons pris connaissance de la forme de nos deux variables BLOC et TMI, essayons de réaliser une opération statistique de plus haut niveau : le test d'une hypothèse. Nous savons grâce aux analyses précédentes que la mortalité infantile *semble* plus élévée en général dans les pays socialistes que dans les pays capitalistes. Mais pouvons nous le *prouver" de façon rigoureuse en utilisant les règles de l'art ?

Bien évidemment, ce n'est pas l'apprentissage de R qui va vous apprendre la théorie des tests statistiques. Mais R va vous permettre très rapidement de mettre en oeuvre les tests que vous aurez appris en cours ou dans les manuels. Et surtout, il va vous offrir une gamme de possibilités de tests et de variantes qui devrait vous pousser à renforcer simultanément vos connaissances en R et en statitiques. 

Dans l'exemple qui nous intéresse, les étudiants ayant une formation de base en statistique savent qu'il existe un test d'égalité des moyennes de deux échantillons appelé test t de student qui semble approprié au problème de comapraison de la mortalité infantile des pays socialistes et capitalistes. Il tient compte de l'effectif de ces échantillons (combiens de pays de chaque type ?) et de leur hétérogénéité interne (appelée variance). On devine intuitivement que la différence des moyennes de deux échantillons sera d'autant plus significative qu'elle oppose deux groupes homogènes comportant chacun de nombreux pays. 

Si cela ne vous parait pas intuitif, imaginez un premier groupe de trois pays ayant pour mortalité (10, 20, 30) et un second groupe de trois pays également ayant pour mortalité (5,15,25). Leurs moyennes respectives sont 20 et 15, soit un écart de 5. Mais il semblerait hasardeux de conclure à de réelles différences compte tenu de l'étalement de chaque distribution. 
```{r}
# Comparaison de la distribution de deux échantillons
VARQUANT<-c(10,20,30,5,15,25)
VARQUALI<-c("A","A","A","B","B","B")
par(mfrow=c(1,1))
boxplot(VARQUANT~ VARQUALI)
```


Imaginez maintenant un premier groupe de 6 pays ayant pour mortalité (18,19,20,20,21,22) et un second groupe de 10 pays ayant pour mortalité (13,14,15,15,16,17). Les moyennes des deux groupes sont toujours respectivement de 20 et de 15, mais on "sent" que les écarts sont plus pertinents car ils portent sur deux groupes plus larges et surtout plus homogènes. Dit autrement, il est peu probable que les différences observées dans la seconde expérience soient le fruit du hasard alors que, dans le premier cas, on pouvait imaginer que les pays des deux groupes n'étaient pas fondamentalement différents.


```{r}
# Comparaison de la distribution de deux échantillons
VARQUANT<-c(18,19,20,20,21,22,13,14,15,15,16,17)
VARQUALI<-c("A","A","A","A","A","A","B","B","B","B","B","B")
par(mfrow=c(1,1))
boxplot(VARQUANT~ VARQUALI)

```


Le test T de student dont vous trouverez la formule dans tous les bons manuels va permettre de transformer l'intuition précédente en mesure objective et en donner une mesure fondamentale appelée **p-value** ou *seuil de rejet de l'hypothèse d'indépendance entre deux caractères*. Là encore, on peut donner une idée intuitive de ce qu'est la p-value en la présentant comme une mesure de la significativité de la relation entre deux caractères. De façon plus rigoureuse, la p-value mesure la probabilité que la relation observée entre deux variables (ici, la différence de moyenne) soit l'effet du hasard. La p-value varie entre 0 et 1, 0 signifiant que la significativité de la relation est certaine (la différence de moyenne ne peut pas être l'effet du hasard) et 1 signifiant qu'il n'existe aucune relation entre les deux variobles (les deux moyennes sont égales).

Examinons brièvement les valeurs de p-value sur nos deux exemples :

```{r}
# Test paramétrique d'égalité des moyennes de Student
VARQUANT<-c(10,20,30,5,15,25)
VARQUALI<-c("A","A","A","B","B","B")
 t.test(VARQUANT ~ VARQUALI) 
```

Vous pouvez lire dans le résultat que la p-value est de 0.5734 ce qui signifie que l'on aurait 57% de chances de se tromper si l'on affirmait que la différence de moyenne entre les deux échantillons A et B est l'effet du hasard. Voyons ce qu'il en est pour notre second exemple

```{r}
# Test paramétrique d'égalité des moyennes de Student
VARQUANT<-c(18,19,20,20,21,22,13,14,15,15,16,17)
VARQUALI<-c("A","A","A","A","A","A","B","B","B","B","B","B")
t.test(VARQUANT ~ VARQUALI) 

```

Cette fois-ci, vous pouvez voir que la valuer du t de student est beaucoup plus forte (6.13) ce qui se traduit par une p-value très proche de zéro (0.00011). La probabilité que les différences entre les deux échantillons soit l'effet du hasard est donc minime et on peut conclure avec une quasi-certiude que A et B correpondent à des groupes de pays ayant des mortalité significativement différentes. 

Essayons maintenant d'appliquer ce même test de Student à notre exemple empirique de la mortalité infantile des pays socialistes et capitalistes.


```{r}
# Test paramétrique d'égalité des moyennes de Student (inadapté)
 t.test(euro$TMI ~ euro$BLOC) 

```

Cette fois-ci le t de Student est égal à -3.19 et la p-value est égale à 0.0147, ce qui signifie que nous avons environ 1.5% de chances que les différences observées entre pays socialistes et capitalistes soient le simple effet du hasard et ne reflètent pas une véritable opposition. Du coup, que faut-il conclure ? 

En sciences sociales, il est de tradition de considérer comme significatives les relations comportant un risque d'erreur inférieur à 5% soit 0.05. Puisque notre p-value est inférieure à 0.05, nous pouvons donc nous appuyer sur cette tradition et écrire que *"l'on peut affirmer avec un risque d'erreur inférieur à 5% que les pays socialistes et capitalistes présentaient des différences significatives de mortalité infantile en 1988"*. 

Ou bien écrire plus simplement *"En 1988 en Europe, la mortalité infantile moyenne des pays socialistes (21.2 P.1000) était significativement plus forte que celles des pays capitalistes (9.1 p.1000)"* et ajouter dans une note en bas de page la remarque qui fait chic *p-value=0.0147*. 


Ne concluez cependant pas trop vite que vous maitrisez désormais toutes les subtilités du test statistique d'égalité des moyennes. Si notre raisonnement précédent est *approximativement* juste, il serait sans doute critiqué par un vrai statisticien qui noterait immédiatement que le test t de student n'est pas le plus adapté à notre problème car (1) les distributions de mortalité ne sont pas gaussiennes et comportent des valeurs exceptionnelles comme l'Albanie pour les pays socialites ou le Portugal pour les pays capitalistes et (2) la variance des deux échantillons (leur hétérogénéité interne) n'est pas égale ce qui peut fausser l'usage du test de Student. Dans le cas présent, un bon statisticien préférerait sans doute utiliser un test non paramétrique, fondé sur la comparaison de l'ordre des observations et non pas leur valeur. 

Soit par exemple le test de Wilcoxon, qu'il est très facile de mettre en oeuvre sour R en changeant à peine notre programme. 

```{r}
# Test non paramétrique d'égalité des médianes de Wilcoxon (correct)
 wilcox.test(euro$TMI~euro$BLOC) 

```

LA p-value est désormais égale à 0.0001 et on peut conclure avec certitude à l'existence de différence entre nos deux échantillons, ce qui n'était pas le cas avec le test de student, moins adapté au cas étudié. 

## Expliquer une variable quantitative (Y) par une variable quantitative (X)

La mortalité infantile (Y) peut-elle être estimée à l'aide du PIB/habitant (X) ? 

Cette exemple de mise en relation de deux variables quantitatives permet de passer rapidement en revue les principe de base de l'analyse des corrélations entre plusieurs variables et de la réalisation d'un modèle d'ajustement linéaire. Il faut toutefois noter que l'exemple a surtout des vertus pédagogiques car le PNB/hab. des pays socialistes d'Europe en 1988 n'était pas véritablement connu et reposait sur des estimations. 


### Calcul des coefficients de corrélations

La façon la plus simple de calculer un coefficient de corrélation est d'appliquerla fonction *cor* à un tableau ne contenant que des variables quantitatives

```{r}
# Coefficients de corrélation de pearson
tab<-euro[,c("TMI","PNB","URB","FEC","POP")]
cor(tab)
```

R calcule par débaut le coefficient de corrélation linéaire de Bravais-Pearson qui varie entre -1 (corrélation négative parfaite) et +1 (corrélation positive parfaite). On peut ainsi voir que le taux de mortalité infantile (TMI) a une très forte corrélation positive avec l'indice conjonctuel de fécondité (+0.81) et une forte corrélation négative avec le PNB/hab (-0.68) ou le taux d'urbanisation (-0.65). La corrélationavec la population du pays est négative mais faible (-0.14).

### Visualisation des nuages de points

Une petite fonction graphique appelée `pairs()`permet de visualiser facilement les différents nuages de points correspondant à chaque paire de variable. On ajuste la taille de la sortie graphique avec `fig.width = ` et `fig.height = ` dans l'en-tête du chunk .

```{r, fig.width = 6 , fig.height= 6 }
pairs(tab)
```






### Test de significativité d'un coefficient de corrélation

Cette procédure n'est cependant pas totalement convaincante d'un point de vue statistique car elle n'affiche pas le degré de *significativité* de la liaison statistique, la fameuse *p-value* dont nous avons discuté dans l'exemple précédent. Cette significativité dépend en effet à la fois (1) de la valeur absolue du coefficient de corrélation (plus on se rapproche de +1 ou -1,plus la relation est significative) mais aussi (2) de la taille de l'échantillon utilisé pour calculer cette corrélation, appelée nombre de degrés de liberté. Lorsqu'on calcule un coefficient de corrélation, le nombre de dégré de liberté est égal à (N-2) c'est-àdire au nombre d'observations (N) moins le nombre de paramètres utilisés pour faire le calcul (soit 2, puisqu'on a besoin de la moyenne de X et de celle de Y).

Pour savoir si une relation est significative, il faut procéder à un test statistique à l'aide de la fonction *cor.test*. On va par exemple tester la significativité des relations entre TMI et les quatre autres variables. 


```{r, message=FALSE,}
# Test du coefficient de corrélation de Peason

cor.test(tab$TMI,tab$PNB, type = "pearson")
cor.test(tab$TMI,tab$URB, type = "pearson")
cor.test(tab$TMI,tab$FEC, type = "pearson")
cor.test(tab$TMI,tab$POP, type = "pearson")

```

On voit ainsi apparaître deux tableaux, l'un avec les coefficients de corrélation des variables et l'autre avec les p-value associées à chacun de ces coefficients. Il apparaît alors que les corrélations du taux de mortalité infantile sont très significatives (<0.001) avec le PNB/hab, le taux d'urbanisation et l'indice conjoncturel de fécondité. Mais que la corrélation entre mortalité infantile et population est non-significative. 

Ajoutons maintenant une autre précaution statistique consistant à calculer un autre coefficient de corrélation, le coefficient de corrélation de rang aussi appelé coefficient de Spearman. L'intérêt de ce coefficient est tout d'abord de limiter le jeu des valeurs exceptionelles (**outlier**) qui sont liées à la position excentré d'une seule observation. Mais c'est aussi plus généralement de mettre en évidence des **relations croissantes ou décroissantes de forme non-linéaire** entre deux variables. En présence de ces deux types de singularité, le coefficient de Spearman affiche souvent des valeurs très divergentes de celui de Pearson. Et la découverte d'une discordance entre les deux coefficients est un indice à ne pas négliger.  


```{r}
# Coefficient de corrélation de Spearman
cor(tab, method = "spearman")

# Test du cofficient de Spearman
cor.test(tab$TMI,tab$PNB, method = "spearman")
cor.test(tab$TMI,tab$URB, method = "spearman")
cor.test(tab$TMI,tab$FEC, method = "spearman")
cor.test(tab$TMI,tab$POP, method = "spearman")

```

Dans l'exemple (ancien mais pédagogique) qui a été choisi, il y a clairement une singularité dans la relation entre TMI et PNB puisque la corrélation qui était de (+0.68) avec le coefficient de Pearson grimpe à (+0.90) avec celui de Spearman. Dans le même temps, la relation entre TMI et FEC chute de (+0.81) à (+0.51) ce qui indique une autre singularité. Il faudra donc examiner soigneusement les nuages de points pour comprendre d'où viennent ces divergences, ce que l'on va faire au cours de l'étape suivante. 

### Visualisation de la forme de la relation entre deux variables quantitatives

On peut visualiser une relations entre deux variables X et Y à l'aide de l'instruction générique *plot()*. Soit par exemple la relation entre urbanisation et mortalité infantile :


```{r}
plot(euro$URB,euro$TMI)
```

On peut également ranger plusieurs graphiques en une seule figure en utilisant l'instruction générale *par* qui définit le format des graphiques, associée au paramètre *mfrow* qui indique combien de lignes et de colonnes on souhaite générer. Chacune des case de ce tableau servira successivement à afficher les graphiques demandés par l'instruction plot. Ici, nous déclarons que nous voulons ordonner les graphiques sur 2 lignes et 2 colonnes puis nous executons 4 fois de suite l'instruction plot.


```{r}
# Un seul graphique de synthèse
par(mfrow= c(2,2)) 
plot(euro$PNB,euro$TMI)
plot(euro$URB,euro$TMI)
plot(euro$FEC,euro$TMI)
plot(euro$POP,euro$TMI)

```

Ces figures permettent (avec un peu d'habitude ...) d'expliquer les divergences observées entre les coefficients de corrélation de Pearson et de Spearman. 

```{r}
par(mfrow= c(1,2)) 

X<-euro$PNB
Y<-euro$TMI

corlin<-round(cor(X,Y),3)
reglin<-lm(Y~X)
plot(X,Y, main=paste("Coeff Pearson = ",corlin))
abline(reglin,col="red")

rankX<-rank(X)
rankY<-rank(Y)
corrank<-round(cor(rankX,rankY),3)
regrank<-lm(rankY~rankX)
plot(rankX,rankY, main=paste("Coeff Spearman = ",corrank))
abline(regrank,col="red")

```


- la relation entre TMI et PNB est clairement non linéaire et forme un arc de parabole plutôt qu'une droite. C'est la raison pour laquelle le coefficient de Spearman était beaucoup plus élevé que celui de Pearson. En effet, lorsque l'on transforme les variables en rang, la courbure disparaît.

```{r}
par(mfrow= c(1,2)) 

X<-euro$FEC
Y<-euro$TMI

corlin<-round(cor(X,Y),3)
reglin<-lm(Y~X)
plot(X,Y, main=paste("Coeff Pearson = ",corlin))
abline(reglin,col="red")

rankX<-rank(X)
rankY<-rank(Y)
corrank<-round(cor(rankX,rankY),3)
regrank<-lm(rankY~rankX)
plot(rankX,rankY, main=paste("Coeff Spearman = ",corrank))
abline(regrank,col="red")

```


- la relation entre TMI et FEC est quant à elle influencée par une valeur exceptionnelle, l'Albanie, qui affiche une fécondité et une mortalité infantile excpetionnnelles. Le point Albanie est situé très en dehors du nuage des autres points et il fait fortement augmenter le coefficient de Pearson. Mais si l'on passe en rang, son effet disparaît et le nuage de point apparaît beaucoup moins linéaire, ce qui explique la valeur plus faible du coefficient de Spearman. 

**Le point important à retenir** (en l'absence de la lecture d'un manuel de statistique plus précis) est qu'on ne doit pas tenter de faire passer une droite dans un nuage de points si celui-ci ne forme pas un ensemble de points régulièrement alignés. Il faut éviter d'utiliser la régression linéaire sur un nuage de points ayant une courbure ou sur un nuage de points comportant des valeurs exceptionnelles. On va voir dans la section suivante quelques "trucs" permettant de corriger la forme d'un nuage de point inadapté à une analyse de régression linéaire (ce qui ne dispense pas de lire un manuel de statistique expliquant ces trucs à l'aide des concepts plus précis d'autocorrélation des résidus, d'hétéroscédasticité, etc...)

### Un premier modèle de régression linéaire (quick & dirty ...)

Le calcul d'une régression linéaire se fait très facilement avec l'instruction **lm** qui est l'acronyme de *linear model*. La particularité de R par rapport à d'autres logiciels est de stocker les résultats d'une régression (et plus généralement d'un modèle statistique quelconque) dans un objet dont on choisit le nom (ici : *MonModele*). C'est objet est uen sorte de grand sac sans fonds contenant des dizaines de résultats que l'on peut sortir un par un en fonction de ses besoins et surtout de ses connaissances en statistiques... Les débutants se limiteront à demander de sortir du sac à malice un résumé (*summary*) de la régression qu'ils viennent d'opérer :

```{r}
MonModele <- lm(euro$TMI~euro$PNB)
summary(MonModele)
```

On trouve dans ce résumé l'essentiel à savoir l'équation de la droite de régression linéaire dans la partie coefficient (TMI = -0.00114 * PNB + 21.34), le pouvoir explicatif du modèle qui est le carré du coefficient de corrélation appelé en anglais R-Squared (0.457 soit 46%) et la significativité générale du modèle qui est fournie par la p-value (0.00028). Cette dernière est en l'occurence conforme au coefficient de corrélation entre TMI et PNB car nous faisons une régression simple. On verra ultérieurement le cas des régressions multiples. 

La visualisation de cette droite de régression peut se faire très simplement en traçant le nuage de point avec **plot** et en ajoutant la droite en utilisant l'instruction **abline** appliquée au sac à malice où se trouvent les résultats de notre modèle. Même si nous ignorons le contenu détaillé de ce sac, nous savons que R va y trouver les éléments utiles à savoir les coefficients de la droite de régression que nous avons déjà vu dans le résumé. On va faire un petit effort d'habillage en ajoutant à la figure un titre principal (*main*) et des titres d'axes (*xlab,ylab*). 

```{r}
par(mfrow=c(1,1))
plot(euro$PNB,euro$TMI,
     main=" Richesse et mortalité infantile en Europe vers 1988",
     ylab="PNB en $ par habitant (1988)",
     xlab="Taux de mortalité infantile en p. 1000 (1988)") 
abline(MonModele, col="red")
```

Nous avons réussi à faire notre premier modèle de régression sous R ! Mais la vérité oblige à dire qu'il n'est pas satisfaisant puisque nous avons ajusté une droite dans un nuage de point qui est manifestement de forme non linéaire. Essayons de faire mieux, en montrant que la question statistique peut recouper des question plus théoriques de géographie économique et de géographie politique. 

### Un deuxième modèle plus satisfaisant sur le plan statistique et théorique

La courbure de la relation entre PNB/habitant et taux de mortalité infantile est certes un problème statistique. Mais elle soulève aussi une question théorique plus intéressante : *Est-il raisonnable et logique de penser que plus le PNB/hab. augmente, plus la mortalité infantiel décroît de façon linéaire ?*

Notre modèle précédent (Y=aX+b) est en partie absurde puisqu'il nous dit que la mortalité infantile minimale est de 21.34°/°° dans un pays ayant un PNB nul (on sait qu'il existe des mortalité infantile plus forte) et surtout il suppose que chaque fois que le PNB augmente de 1000 \$, la mortalité infantile baisse de 1.1 °/°°. Du coup, ce modèle prévoit une absence totale de mortalité infantile pour les pays ayant un revenu supérieur à 15000 \$/hab. Et il annonce froidement des **mortalités infantile négatives** au delà de ce niveau de richesse ! Bref, il y a une inconsistance logique dans un tel modèle, indépendamment du problème statistique de non linéarité. 

Or, ce que nous disent les théoriciens du développement comme A. Sen est l'existence de rendements décroissant dans les progrès au fur et à mesure que la richesse augmente. En d'autres termes, la mortalité infantile diminue rapidement quant on passe de l'extrême apuvreté à la pauvreté. Mais ensuite elle baisse de moins en moins vite et son effet devient négligeable au delà d'un seuil de richesse. Le modèle correspondant à ces hypothèses théoriques est donc un modèle de décroissance non linéaire.

Il peut s'agir d'une loi exponentielle négative qui peut s'écrire Y = b.exp(-aX), ce que l'on peut transformer en équation linéaire en le réécrivant sous la forme log(Y) = log(b)+aX. Ou bien d'une loi puissance négative qui peut s'écrire Y = b.X^a et se transforme en équation linéaire log(y) = a.log(X)+log(b). Si l'on n'a pas de préférence théorique pour l'une ou l'autre de ces lois, on peut simplement examiner celle qui donne le meilleur ajustement linéaire après transformation logarithmique :

```{r}
par(mfrow= c(1,2)) 

X<-euro$PNB
Y<-euro$TMI

corexp<-round(cor(X,log(Y)),3)
regexp<-lm(log(Y)~X)
plot(X,log(Y), main=paste("Modèle exponentiel : r = ",corexp))
abline(regexp,col="red")

corpui<-round(cor(log(X),log(Y)),3)
regpui<-lm(log(Y)~log(X))
plot(log(X),log(Y), main=paste("Modèle puissance : r = ",corpui))
abline(regpui,col="red")

```

La comparaison des coefficients de corrélation mais aussi l'examen de la forme des nuages de points plaide clairement en faveur du choix du modèle puissance. Il demeure en effet une nette courbure du nuage de points dans le graphique du modèle exponentiel et l'ajustement obtenu est plus faible. Le modèle puissance affiche un alignement presque parfait des points avec une corrélation très élevée. Donc, *si l'on ne dispose pas d'arguments théoriques ou empiriques particuliers en faveur du modèle exponentiel*, on retiendra le modèle puissance pour modéliser la relation entre mortalité infantile et PNB/habitant des pays européens en 1988.

### Un troisième modèle : la revanche des pays socialistes ...

Comme nous savons désormais (1) que la mortalité infantile varie fortement entre les pays socialistes et capitalistes et (2) que la mortalité infantile dépend de la richesse par habitant, on peut proposer un dernier modèle qui va faire intervenir simultanément les variables. Il existe de nombreuses variantes pour construire un tel modèle que vous apprendrez dans les cours sur la régression multiple ou l'analyse multi-niveaux. On se limitera ici à une approche plus intuitive qui ne présuppose pas encore de connaître des techniques statistiques poussées.

Examinons tout d'abord la relation TMI*PNB dans les pays socialistes :

```{r}
par(mfrow=c(1,1))
summary(euro$PNB)
eurosoc<-euro[euro$BLOC=="Soc",]
MonModele2soc <- lm(eurosoc$TMI~eurosoc$PNB)
summary(MonModele2soc)
plot(eurosoc$PNB,eurosoc$TMI,
                           xlim=c(0,6000),
                            ylim=c(0,50),
                           main=" Pays socialistes",
                           ylab="PNB en $ par habitant (1988)",
                           xlab="Taux de mortalité infantile en p. 1000 (1988)") 
abline(MonModele2soc, col="red")
```

La relation semble bien linéaire avec une valeur initiale forte de mortalité infantile (coefficient b=40.4) et une décroissance rapide de la mortalité infantile lorsque le PNB augmente (coefficient a = -0.009). La mortalité infantile baisse de 9 points chaque fois que le PNB augmente de 1000 $. On peut estimer qu'au delà d'un PNB de 5000 $ environ, la mortalité infantile n'existera plus dans les pays socialistes.


Examinons maintenant séparément la situation dans les pays capitalistes
```{r}
par(mfrow=c(1,1))
eurocap<-euro [euro$BLOC=="Cap",]
MonModele2cap <- lm(eurocap$TMI~eurocap$PNB)
summary(MonModele2cap)
plot(eurocap$PNB,eurocap$TMI,
    xlim=c(0,20000),
    ylim=c(0,20),
     main=" Pays capitalistes",
     ylab="PNB en $ par habitant (1988)",
     xlab="Taux de mortalité infantile en p. 1000 (1988)") 
abline(MonModele2cap, col="blue")

```

La relation semble à nouveauà peu près linéaire. La valeur initiale de mortalité infantile est plus faible que dans les pays socialistes (coefficient a = 13.1) mais ensuite la décroissance est lente lorsque le PNB augmente (coefficient a = -0.0004). La mortalité infantile ne baisse que de 0.4 points lorsque le PNB augmente de 1000 $. Il faudrait donc un PNB d'environ 30000 $ par habitant pour que la mortalité infantile disparaissent dans les pays capitalistes, alors qu'il suffisant de 5000 $ par habitant dans les pays socialistes !


On va résumer ce dernier modèle par une seule figure sur laquelle on affichera simultanément les deux droites de régression. 

```{r}
par(mfrow=c(1,1))

plot(euro$PNB,euro$TMI,
    xlim=c(0,20000),
    ylim=c(0,50),
     main="Influence du niveau de vie et du syème politique sur la mortalité infantile",
     xlab="PNB en $ par habitant (1988)",
     ylab="Taux de mortalité infantile en p. 1000 (1988)") 

points(eurosoc$PNB,eurosoc$TMI,col="red")
text(eurosoc$PNB,eurosoc$TMI,eurosoc$PAYS,col="red",cex=0.5,pos=4)
abline(MonModele2soc,col="red")

points(eurocap$PNB,eurocap$TMI,col="blue")
text(eurocap$PNB,eurocap$TMI,eurocap$PAYS,col="blue",cex=0.5,pos=4)
abline(MonModele2cap,col="blue")

```

Selon ce modèle, on pouvait penser en 1988 qu'un pays capitaliste comme le Portugal devrait très fortementaccroître son PNB par habitant pour voir baisser sa mortalité infantile. Inversement, la Pologne qui était un pays socialiste aurait du voir rapidement sa situation s'améliorer, pour peu qu'elle accroisse légèrement son PNB/habitant. Mais on ne saura jamais si le modèle aurait été vérifié puisque le bloc socialiste s'est dissous en 1989 ...


## Exercice 

Essayez de reproduire la même analyse sur deux autres variables quantitatives de votre choix 




# STAT. BIVARIEE : Y Qualitative

On suppose que vous avez acquis les premiers réflexes de programmation en R avec les variables quantitatives contenues dans le fichier Europe 1988. VOus y avez appris à expliquer une variable Y  quantitative par une variable X pouvant être soit qualitative (analyse de variance, test d'égalité des moyennes), soit quantitative (corrélation, régression)

Dans ce deuxième module, on va essayer d'expliquer une variable qualitative Y par une autre variable qualitative X. L'exemple utilisé ici est basée sur une grande enquête internationale réalisée dans plusieurs pays du Monde à intervalles réguliers : la [World Value Survey](https://www.worldvaluessurvey.org/WVSContents.jsp)


## Préparation des données

### Chargement de packages

Pour faciliter l'utilisation des données de ce questionnaire, nouus allons faire appel à un très joli package mis au point par Julien Barnier et Joseph Larmarange : `questionr` et un package plus international `labelled` :

```{r}
library(questionr)
```


 
### répertoire de travail


On vérifie l'emplacement où l'on va trouver les données

```{r}
# (1.2) Choix du rÈpertoire contenant les donnÈes
monchemin<-"data/wvs"
list.files(monchemin)

```


## Préparation des données

Nous allons maintenant essayer de charger le fichier de données qui est au format interne de R avec l'extension .RDS puis sélectionner notre échantillon d'étude

### Charger le tableau complet


```{r}
don<-readRDS("data/wvs/wvs_sample.RDS")
str(don)
```


On affiche les premières lignes (*head*) ou les dernières lignes (*tail*) du tableau pour voir si le chargement s'est bien effectué : 

```{r}
head(don)
tail(don)
```

### Liste des variables

Les variable proposées sont les suivantes :

- **pays** : Pays d'enquête
- **year** : Année d'enquête
- **wgt**  : Pondération (ech. représentatif)
- **sex**  : Sexe
- **age**  : Age
- **gen**  : Génération
- **job_man** : Priorité aux hommes en matière d'emploi lorsque ceux-ci sont rares (*Jobs scarce: Men should have more right to a job than women*)
- **job_nat** : Priorité aux nationaux en matière d'emploi lorsque ceux-ci sont rares (*Jobs scarce: Employers should give priority to (nation) people than immigrants*)



### Sélection d'un pays 

On examine la liste complète des pays pour faire son choix

```{r}
table(don$pays)
```

Puis on en retient un, par exemple l'Allemagne et o n extrait les données : 

```{r}
sel<-don[don$pays == "Germany",]
```


### Recodage des facteurs

Les facteurs doivent être le plus souvent recodés, soit pour éliminer les non-réponses, soit pour fusionner entre elles des modalités qu'on juge équivalentes.


```{r}
levels(sel$sex) <- c(NA,NA,NA,NA, "Homme","Femme")
levels(sel$job_man)<-c(NA,NA,NA,NA,"NSP","D'accord","Pas d'acccord","NSP")
levels(sel$job_nat)<-c(NA,NA,NA,NA,"NSP","D'accord","Pas d'acccord","NSP")
```

### Découpage des classes

Les variables quantitatives peuvent de leur côté être découpées en classes, soit à partir de considérations quantitatives (e.g. classes d'âges de 10 ans) soit dans une perspective plus qualitative (e.g. générations situées avant ou après le Baby Boom)

```{r}
sel$age6<-cut(sel$age, breaks=c(15,29,39,49,59,69,110))
levels(sel$age6)<-c("- de 25 ans",
                    "30-39 ans",
                    "40-49 ans",
                    "50-59 ans",
                    "60-69 ans",
                    "70 ans et +")
sel$gen4<-cut(sel$gen, breaks=c(1901,1944,1959,1974,2010))
levels(sel$gen4)<- c("Pre-Boomer (< 1945)",
                  "Boomer I (1945-1959)",
                  "Boomer II (1960-74)",
                 "Post-Boomer (> 1974)")

```

### Division en deux périodes

Nous avons retenu pour chaque pays deux vagues d'enquêtes : l'une récente et l'autre plus ancienne d'environ une génération

```{r}
sel1<-sel[sel$year <2000,]
sel2<-sel[sel$year>2000,]
```

### Résumé

```{r}
summary(sel1)
summary(sel2)
```

## Evolution de l'opinion au cours du temps

### Création du tableau de contingence

```{r}
X<-sel$year
Y<-sel$job_man
W<-sel$wgt
tc<- wtd.table(X,Y,W)
tc
```

### Profils en pourcentage

```{r}
lprop(tc)
cprop(tc)
prop(tc)
```

### Visualisation

```{r}
plot(tc,col=c("gray","yellow","orange"), main="Priorité au travail des hommes")
```

### Test du chi-2

```{r}
tk2 <- chisq.test(tc)
tk2
```

### Résidus

```{r}
kable(tk2$observed, caption = "Valeurs observées")
kable(tk2$expected, caption = "Valeurs théoriques")
kable(tk2$observed-tk2$expected, caption = "Résidus bruts", digits=1)
kable(tk2$residuals , caption = "Résidus standardisés", digits=2)
```

### Visualisation des écarts signifcatifs

```{r}
mosaicplot(tc, shade =TRUE)
```

## Variation selon le sexe

### En 1997

```{r}
X<-sel1$sex
Y<-sel1$job_man
W<-sel1$wgt
titre <- "Travail des femmes / Sexe (Allemagne, 1997)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")


```

### En 2018

```{r}
X<-sel2$sex
Y<-sel2$job_man
W<-sel2$wgt
titre <- "Travail des femmes / Sexe (Allemagne, 2018)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")

```

## Variation selon l'âge

### En 1997

```{r}
X<-sel1$age6
Y<-sel1$job_man
W<-sel1$wgt
titre <- "Travail des femmes / Âge (Allemagne, 1997)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")

```

### En 2018

```{r}
X<-sel2$age6
Y<-sel2$job_man
W<-sel2$wgt
titre <- "Travail des femmes / Âge (Allemagne, 2018)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")

```

## Variation selon les générations

### En 1997

```{r}
X<-sel1$gen4
Y<-sel1$job_man
W<-sel1$wgt

titre <- "Travail des femmes / Génération (Allemagne, 1997)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")

```

### En 2018

```{r}
X<-sel2$gen4
Y<-sel2$job_man
W<-sel2$wgt
titre <- "Travail des femmes / Génération (Allemagne, 2018)"

# Tableau de contingence
tc<- wtd.table(X,Y,W)
kable(tc, caption= titre)
      


# Pourcentage en ligne
kable(lprop(tc), caption = titre)


# Test du chi2
test <- chisq.test(tc)
test

# Graphiques

plot(tc,col=rainbow(n=10),main=titre, sub = "Fréquences")

mosaicplot(tc,shade=T,main = titre, sub="Résidus")

```



# Bibliographie {-}

<div id="refs"></div>


# Annexes {-}


## Infos session  {-}

```{r session_info, echo=FALSE}
kableExtra::kable_styling(kable(sessionRzine()[[1]], row.names = F))
kableExtra::kable_styling(kable(sessionRzine()[[2]], row.names = F))
```


## Citation {-}

```{r generateBibliography, echo=FALSE}

cat(readLines('cite.bib'), sep = '\n')

``` 

<br>

## Glossaire {- #endnotes}

```{js, echo=FALSE}

$(document).ready(function() {
  $('.footnotes ol').appendTo('#endnotes');
  $('.footnotes').remove();
});

```
